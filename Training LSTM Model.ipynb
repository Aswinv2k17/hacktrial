{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_config\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dimble Scaria\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10380 samples\n",
      "Epoch 1/50\n",
      "10380/10380 [==============================] - 12s 1ms/sample - loss: 66.4933 - mae: 4.4558\n",
      "Epoch 2/50\n",
      "10380/10380 [==============================] - 2s 215us/sample - loss: 41.4275 - mae: 3.6353\n",
      "Epoch 3/50\n",
      "10380/10380 [==============================] - 2s 224us/sample - loss: 34.7268 - mae: 3.5117\n",
      "Epoch 4/50\n",
      "10380/10380 [==============================] - 2s 239us/sample - loss: 31.8159 - mae: 3.4298\n",
      "Epoch 5/50\n",
      "10380/10380 [==============================] - 2s 217us/sample - loss: 29.7785 - mae: 3.2883\n",
      "Epoch 6/50\n",
      "10380/10380 [==============================] - 2s 218us/sample - loss: 28.1123 - mae: 3.1036\n",
      "Epoch 7/50\n",
      "10380/10380 [==============================] - 2s 227us/sample - loss: 25.8135 - mae: 2.9222\n",
      "Epoch 8/50\n",
      "10380/10380 [==============================] - 2s 226us/sample - loss: 22.3295 - mae: 2.7000\n",
      "Epoch 9/50\n",
      "10380/10380 [==============================] - 2s 226us/sample - loss: 19.3965 - mae: 2.4573\n",
      "Epoch 10/50\n",
      "10380/10380 [==============================] - 2s 240us/sample - loss: 18.3087 - mae: 2.3829\n",
      "Epoch 11/50\n",
      "10380/10380 [==============================] - 2s 230us/sample - loss: 16.6621 - mae: 2.2958\n",
      "Epoch 12/50\n",
      "10380/10380 [==============================] - 2s 241us/sample - loss: 14.8036 - mae: 2.1864\n",
      "Epoch 13/50\n",
      "10380/10380 [==============================] - 3s 244us/sample - loss: 14.7324 - mae: 2.1464\n",
      "Epoch 14/50\n",
      "10380/10380 [==============================] - 3s 249us/sample - loss: 14.0320 - mae: 2.1035\n",
      "Epoch 15/50\n",
      "10380/10380 [==============================] - 3s 265us/sample - loss: 13.7162 - mae: 2.0758\n",
      "Epoch 16/50\n",
      "10380/10380 [==============================] - 3s 254us/sample - loss: 13.0831 - mae: 2.0166\n",
      "Epoch 17/50\n",
      "10380/10380 [==============================] - 3s 280us/sample - loss: 12.5333 - mae: 1.9689\n",
      "Epoch 18/50\n",
      "10380/10380 [==============================] - 3s 243us/sample - loss: 11.9817 - mae: 1.9238\n",
      "Epoch 19/50\n",
      "10380/10380 [==============================] - 3s 298us/sample - loss: 12.2711 - mae: 1.9182\n",
      "Epoch 20/50\n",
      "10380/10380 [==============================] - 3s 280us/sample - loss: 12.1057 - mae: 1.9041\n",
      "Epoch 21/50\n",
      "10380/10380 [==============================] - 3s 248us/sample - loss: 11.4218 - mae: 1.8465\n",
      "Epoch 22/50\n",
      "10380/10380 [==============================] - 3s 250us/sample - loss: 11.2766 - mae: 1.8426\n",
      "Epoch 23/50\n",
      "10380/10380 [==============================] - 3s 252us/sample - loss: 10.8467 - mae: 1.8083\n",
      "Epoch 24/50\n",
      "10380/10380 [==============================] - 3s 271us/sample - loss: 10.9234 - mae: 1.8197\n",
      "Epoch 25/50\n",
      "10380/10380 [==============================] - 3s 295us/sample - loss: 10.6535 - mae: 1.7976\n",
      "Epoch 26/50\n",
      "10380/10380 [==============================] - 3s 268us/sample - loss: 11.2327 - mae: 1.8080\n",
      "Epoch 27/50\n",
      "10380/10380 [==============================] - 3s 244us/sample - loss: 10.5565 - mae: 1.7879\n",
      "Epoch 28/50\n",
      "10380/10380 [==============================] - 3s 302us/sample - loss: 10.5883 - mae: 1.7796\n",
      "Epoch 29/50\n",
      "10380/10380 [==============================] - 3s 277us/sample - loss: 10.2331 - mae: 1.7585\n",
      "Epoch 30/50\n",
      "10380/10380 [==============================] - 2s 239us/sample - loss: 9.9065 - mae: 1.7475\n",
      "Epoch 31/50\n",
      "10380/10380 [==============================] - 3s 291us/sample - loss: 10.2175 - mae: 1.7484\n",
      "Epoch 32/50\n",
      "10380/10380 [==============================] - 3s 277us/sample - loss: 9.4661 - mae: 1.7070\n",
      "Epoch 33/50\n",
      "10380/10380 [==============================] - 3s 278us/sample - loss: 10.0582 - mae: 1.7502\n",
      "Epoch 34/50\n",
      "10380/10380 [==============================] - 3s 309us/sample - loss: 9.8456 - mae: 1.7419\n",
      "Epoch 35/50\n",
      "10380/10380 [==============================] - 3s 244us/sample - loss: 10.1483 - mae: 1.7524\n",
      "Epoch 36/50\n",
      "10380/10380 [==============================] - 3s 242us/sample - loss: 9.6082 - mae: 1.7278\n",
      "Epoch 37/50\n",
      "10380/10380 [==============================] - 2s 241us/sample - loss: 9.2040 - mae: 1.6738\n",
      "Epoch 38/50\n",
      "10380/10380 [==============================] - 3s 242us/sample - loss: 10.0830 - mae: 1.7269\n",
      "Epoch 39/50\n",
      "10380/10380 [==============================] - 3s 251us/sample - loss: 9.2361 - mae: 1.6875\n",
      "Epoch 40/50\n",
      "10380/10380 [==============================] - 3s 296us/sample - loss: 9.5309 - mae: 1.7049\n",
      "Epoch 41/50\n",
      "10380/10380 [==============================] - 3s 252us/sample - loss: 9.0587 - mae: 1.6759\n",
      "Epoch 42/50\n",
      "10380/10380 [==============================] - 3s 257us/sample - loss: 9.6162 - mae: 1.7110\n",
      "Epoch 43/50\n",
      "10380/10380 [==============================] - 3s 248us/sample - loss: 9.3748 - mae: 1.6858\n",
      "Epoch 44/50\n",
      "10380/10380 [==============================] - 3s 253us/sample - loss: 9.5258 - mae: 1.7068\n",
      "Epoch 45/50\n",
      "10380/10380 [==============================] - 2s 241us/sample - loss: 9.3113 - mae: 1.6923\n",
      "Epoch 46/50\n",
      "10380/10380 [==============================] - 3s 289us/sample - loss: 9.2359 - mae: 1.6689\n",
      "Epoch 47/50\n",
      "10380/10380 [==============================] - 3s 262us/sample - loss: 8.5929 - mae: 1.6510\n",
      "Epoch 48/50\n",
      "10380/10380 [==============================] - 3s 264us/sample - loss: 9.1897 - mae: 1.6699\n",
      "Epoch 49/50\n",
      "10380/10380 [==============================] - 3s 247us/sample - loss: 8.3913 - mae: 1.6347\n",
      "Epoch 50/50\n",
      "10380/10380 [==============================] - 3s 275us/sample - loss: 9.0284 - mae: 1.6495s - loss: 9.2439 - mae\n",
      "Kappa Score: 0.9606486134376003\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10381 samples\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 8s 731us/sample - loss: 64.1917 - mae: 4.3443\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 2s 238us/sample - loss: 40.8987 - mae: 3.6813\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 2s 241us/sample - loss: 34.7378 - mae: 3.5728\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 2s 236us/sample - loss: 32.4255 - mae: 3.5112\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 2s 241us/sample - loss: 30.0194 - mae: 3.3141\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 2s 235us/sample - loss: 28.4432 - mae: 3.1555\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 3s 249us/sample - loss: 25.0753 - mae: 2.8956\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 3s 262us/sample - loss: 21.1421 - mae: 2.6183\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 2s 237us/sample - loss: 19.2017 - mae: 2.4628\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 2s 241us/sample - loss: 16.9461 - mae: 2.3465\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 3s 244us/sample - loss: 15.6211 - mae: 2.2318\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 2s 229us/sample - loss: 15.3008 - mae: 2.2115\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 2s 238us/sample - loss: 14.4977 - mae: 2.1519\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 3s 270us/sample - loss: 13.6341 - mae: 2.0754\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 13.5953 - mae: 2.0719\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 2s 236us/sample - loss: 12.4292 - mae: 1.9777\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 3s 253us/sample - loss: 12.8535 - mae: 1.9861\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 3s 268us/sample - loss: 11.5876 - mae: 1.8947\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 3s 260us/sample - loss: 11.5401 - mae: 1.8965\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 3s 278us/sample - loss: 11.4751 - mae: 1.8706\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 3s 260us/sample - loss: 11.2424 - mae: 1.8297\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 3s 248us/sample - loss: 10.5838 - mae: 1.8000\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 3s 266us/sample - loss: 10.8357 - mae: 1.8232\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 3s 243us/sample - loss: 10.5800 - mae: 1.7861\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 3s 250us/sample - loss: 10.4423 - mae: 1.7729\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 3s 283us/sample - loss: 9.8165 - mae: 1.7447\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 3s 262us/sample - loss: 9.6406 - mae: 1.7183\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 3s 271us/sample - loss: 9.3004 - mae: 1.7109\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 3s 248us/sample - loss: 9.3079 - mae: 1.6959\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 9.5540 - mae: 1.7310\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 3s 321us/sample - loss: 9.9566 - mae: 1.7435\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 3s 300us/sample - loss: 9.4712 - mae: 1.7100\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 9.2699 - mae: 1.6978\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 3s 255us/sample - loss: 8.9257 - mae: 1.6842\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 3s 266us/sample - loss: 9.0736 - mae: 1.6826\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 3s 298us/sample - loss: 8.6034 - mae: 1.6568\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 3s 327us/sample - loss: 8.6581 - mae: 1.6531\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 3s 326us/sample - loss: 8.9034 - mae: 1.6497\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 2s 239us/sample - loss: 9.0360 - mae: 1.6730\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 2s 241us/sample - loss: 9.1921 - mae: 1.6754s - los\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 2s 241us/sample - loss: 8.9548 - mae: 1.6550\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 2s 236us/sample - loss: 9.0229 - mae: 1.6547\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 3s 274us/sample - loss: 8.3684 - mae: 1.6430\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 3s 259us/sample - loss: 8.6629 - mae: 1.6348\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 2s 239us/sample - loss: 8.3352 - mae: 1.6139\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 3s 247us/sample - loss: 8.3778 - mae: 1.6177\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 3s 255us/sample - loss: 8.3922 - mae: 1.6291\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 2s 236us/sample - loss: 8.4858 - mae: 1.6111\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 3s 253us/sample - loss: 8.1257 - mae: 1.6010\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 8.6836 - mae: 1.6180\n",
      "Kappa Score: 0.955532643248834\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10381 samples\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 666us/sample - loss: 65.2533 - mae: 4.4206\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 41.1840 - mae: 3.6418\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 3s 275us/sample - loss: 34.4005 - mae: 3.5344\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 3s 275us/sample - loss: 31.9593 - mae: 3.4812\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 3s 263us/sample - loss: 30.0619 - mae: 3.2936\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 28.2414 - mae: 3.1454\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 3s 262us/sample - loss: 26.3313 - mae: 2.9716\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 2s 238us/sample - loss: 21.9099 - mae: 2.7208\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 2s 226us/sample - loss: 19.6289 - mae: 2.5622\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 2s 229us/sample - loss: 17.6130 - mae: 2.3920\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 2s 227us/sample - loss: 15.6139 - mae: 2.2731\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 2s 227us/sample - loss: 15.3957 - mae: 2.2108\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 14.3405 - mae: 2.1357\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 3s 253us/sample - loss: 14.3934 - mae: 2.1267\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 2s 230us/sample - loss: 13.0149 - mae: 2.0521\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 2s 238us/sample - loss: 12.9601 - mae: 2.0172\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 3s 241us/sample - loss: 13.1114 - mae: 2.0015\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 2s 233us/sample - loss: 11.8374 - mae: 1.9116\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 2s 239us/sample - loss: 12.1515 - mae: 1.9313\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 3s 274us/sample - loss: 11.9217 - mae: 1.9136\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 11.9145 - mae: 1.8859\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 2s 232us/sample - loss: 11.0729 - mae: 1.8399\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 2s 236us/sample - loss: 11.6052 - mae: 1.8472\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 2s 236us/sample - loss: 10.2647 - mae: 1.7757\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 2s 237us/sample - loss: 10.9236 - mae: 1.8087\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 3s 260us/sample - loss: 10.8580 - mae: 1.7907\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 3s 263us/sample - loss: 10.3595 - mae: 1.7885\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 2s 239us/sample - loss: 10.5865 - mae: 1.8101\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 9.9973 - mae: 1.7427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 9.8831 - mae: 1.7529\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 2s 236us/sample - loss: 9.3139 - mae: 1.7326\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 3s 247us/sample - loss: 9.9754 - mae: 1.7592\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 3s 279us/sample - loss: 9.9059 - mae: 1.7438\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 3s 277us/sample - loss: 9.6163 - mae: 1.7222\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 3s 269us/sample - loss: 9.7658 - mae: 1.7280\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 3s 269us/sample - loss: 10.0575 - mae: 1.7264\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 3s 253us/sample - loss: 9.7630 - mae: 1.7190\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 3s 304us/sample - loss: 9.7910 - mae: 1.7250\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 3s 293us/sample - loss: 9.3763 - mae: 1.7151\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 3s 267us/sample - loss: 9.1970 - mae: 1.7004\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 3s 248us/sample - loss: 8.6217 - mae: 1.6740\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 3s 249us/sample - loss: 9.5041 - mae: 1.6975\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 2s 241us/sample - loss: 9.4666 - mae: 1.7280\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 3s 315us/sample - loss: 9.4828 - mae: 1.7064\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 3s 260us/sample - loss: 9.0412 - mae: 1.6818\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 2s 230us/sample - loss: 9.1197 - mae: 1.6960\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 2s 229us/sample - loss: 8.9970 - mae: 1.6818\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 2s 229us/sample - loss: 8.7209 - mae: 1.6736\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 2s 226us/sample - loss: 8.8845 - mae: 1.6792\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 3s 280us/sample - loss: 8.5485 - mae: 1.6578\n",
      "Kappa Score: 0.9568612355673534\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10381 samples\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 667us/sample - loss: 66.2424 - mae: 4.4214\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 3s 269us/sample - loss: 41.3080 - mae: 3.6128\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 3s 253us/sample - loss: 34.6062 - mae: 3.5165\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 3s 257us/sample - loss: 31.2155 - mae: 3.4122\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 3s 250us/sample - loss: 29.8350 - mae: 3.2724\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 3s 283us/sample - loss: 27.7311 - mae: 3.0954\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 3s 259us/sample - loss: 25.5510 - mae: 2.9218\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 3s 248us/sample - loss: 22.8164 - mae: 2.7294\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 3s 253us/sample - loss: 19.5957 - mae: 2.5063\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 3s 247us/sample - loss: 17.1885 - mae: 2.3522\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 16.2790 - mae: 2.2850\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 3s 279us/sample - loss: 15.4342 - mae: 2.1962 - loss: 15.4083 - mae: \n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 3s 266us/sample - loss: 14.3510 - mae: 2.1484\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 3s 247us/sample - loss: 14.0315 - mae: 2.1040\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 13.1617 - mae: 2.0433\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 13.2758 - mae: 2.0263\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 3s 251us/sample - loss: 13.1951 - mae: 2.0145\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 3s 275us/sample - loss: 11.9293 - mae: 1.9316\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 3s 265us/sample - loss: 12.0579 - mae: 1.9400\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 3s 262us/sample - loss: 11.9907 - mae: 1.9016\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 3s 268us/sample - loss: 11.5805 - mae: 1.8695\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 3s 272us/sample - loss: 11.3088 - mae: 1.8476\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 3s 254us/sample - loss: 10.7971 - mae: 1.8151\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 3s 285us/sample - loss: 10.7173 - mae: 1.7971\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 3s 265us/sample - loss: 10.8958 - mae: 1.8076\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 3s 250us/sample - loss: 10.5825 - mae: 1.7887\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 10.5091 - mae: 1.7811\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 10.1297 - mae: 1.7694\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 3s 248us/sample - loss: 10.3238 - mae: 1.7780\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 3s 281us/sample - loss: 10.1837 - mae: 1.7719\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 3s 265us/sample - loss: 9.7687 - mae: 1.7403\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 3s 259us/sample - loss: 9.8883 - mae: 1.7407\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 3s 263us/sample - loss: 9.9633 - mae: 1.7524\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 9.7678 - mae: 1.7158\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 2s 235us/sample - loss: 9.7356 - mae: 1.7268\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 3s 268us/sample - loss: 8.9740 - mae: 1.6883s - loss: 8.9800 - m\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 9.3952 - mae: 1.7069\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 2s 235us/sample - loss: 9.6946 - mae: 1.7311\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 2s 235us/sample - loss: 9.1126 - mae: 1.6876\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 2s 237us/sample - loss: 9.2025 - mae: 1.6928\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 9.1828 - mae: 1.6958\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 3s 262us/sample - loss: 9.3927 - mae: 1.6809\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 3s 254us/sample - loss: 9.1068 - mae: 1.6921\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 2s 238us/sample - loss: 8.8749 - mae: 1.6714\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 2s 241us/sample - loss: 9.1961 - mae: 1.6865\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 8.7138 - mae: 1.6664\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 9.3033 - mae: 1.6833\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 3s 259us/sample - loss: 8.9692 - mae: 1.6736\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 3s 301us/sample - loss: 8.5600 - mae: 1.6423\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 8.9964 - mae: 1.6567\n",
      "Kappa Score: 0.9588766382248466\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10381 samples\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 677us/sample - loss: 65.1674 - mae: 4.3891\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 3s 241us/sample - loss: 40.5154 - mae: 3.6336\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 34.1576 - mae: 3.4954\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 30.7515 - mae: 3.3916\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 29.6115 - mae: 3.2239\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 27.1996 - mae: 3.0399\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 3s 274us/sample - loss: 24.8407 - mae: 2.8553\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 20.8982 - mae: 2.6013\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 18.3499 - mae: 2.4120\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 3s 243us/sample - loss: 17.1033 - mae: 2.3434\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 16.5544 - mae: 2.2752\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 3s 243us/sample - loss: 14.8099 - mae: 2.1764\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 3s 269us/sample - loss: 14.4337 - mae: 2.1424\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 3s 253us/sample - loss: 13.7575 - mae: 2.0806\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 13.2073 - mae: 2.0320\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 13.1450 - mae: 2.0061\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 12.4665 - mae: 1.9498\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 3s 249us/sample - loss: 12.1400 - mae: 1.9410\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 3s 275us/sample - loss: 11.7606 - mae: 1.9064\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 3s 266us/sample - loss: 11.8992 - mae: 1.9108\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 3s 250us/sample - loss: 11.8554 - mae: 1.8838\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 10.8387 - mae: 1.8197\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 11.5860 - mae: 1.8478\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 10.7769 - mae: 1.8031\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 3s 265us/sample - loss: 10.4909 - mae: 1.7802\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 3s 260us/sample - loss: 11.0014 - mae: 1.7977\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 3s 248us/sample - loss: 10.7427 - mae: 1.7836\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 10.4997 - mae: 1.7657\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 3s 250us/sample - loss: 9.7177 - mae: 1.7346\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 10.3673 - mae: 1.7629\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 3s 266us/sample - loss: 10.0457 - mae: 1.7263\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 3s 289us/sample - loss: 9.6179 - mae: 1.7234\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 9.8130 - mae: 1.7238\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 10.2435 - mae: 1.7526\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 9.9146 - mae: 1.7268\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 9.4347 - mae: 1.6997\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 3s 260us/sample - loss: 8.9121 - mae: 1.6854\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 3s 263us/sample - loss: 9.4457 - mae: 1.7163\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 9.2956 - mae: 1.6956\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 9.6395 - mae: 1.7139\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 9.1237 - mae: 1.6906\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 3s 246us/sample - loss: 9.0966 - mae: 1.6703\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 3s 254us/sample - loss: 9.0268 - mae: 1.6670\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 3s 271us/sample - loss: 8.9294 - mae: 1.6738\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 3s 242us/sample - loss: 9.0472 - mae: 1.6718\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 8.8166 - mae: 1.6580\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 3s 245us/sample - loss: 9.0982 - mae: 1.6801\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 3s 244us/sample - loss: 8.5273 - mae: 1.6425\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 3s 248us/sample - loss: 9.1044 - mae: 1.6689\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 3s 276us/sample - loss: 8.4986 - mae: 1.6355\n",
      "Kappa Score: 0.9620187788341723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(len(X), n_folds=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv:\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train.values, batch_size=64, epochs=50)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avg. Kappa Score is 0.961 which is the highest we have ever seen on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.9613\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2595"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_model.predict(testDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
